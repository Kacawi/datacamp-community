---
title: "Scikit-Learn Tutorial: Python Machine Learning"
output:
  html_document:
    self_contained: false
---

```{r, include=FALSE}
tutorial::go_interactive()
```

## Aprendizagem de Máquina com Python

O Machine learning, em português Aprendizado de Máquina, é um ramo da ciência da computação que estuda do desenvolvimento de algoritmos que podem aprender.

Assim, as tarefas relativas ao Machine Learning são, típicamente, conceitos de aprendizagem, funções de aprendizem ou "modelagem preditiva", clusterização e reconhecimento preditivo de padrões. Essa tarefas são aprendidas pela avaliação de dados, que são observadas através de experiências ou instruções dada aos algoritmos, por exemplo.

Espera-se que os algoritmos de Machine learning incluam, eventualmente, em sua jornada de apredizagem, experiências que aperfeiçoem gradativamente seu aprendizado. Além disso, espera-se que a melhora no aprendizado ocorra de maneira similar ao aprendizado humano, de maneira automática e sem interferências, que nada mais é que o objetivo primordial do Machine learning.

O Machine learning possui bastantes similaridades e compartilha disciplinas com campos como: Descobrimento de conhecimento, mineração de dados, Inteligência Artificial e estatística. Ressalta-se que as aplicação de Machine learning podem ser classificadas em dois campos: o descobrimento de conhecimento cientifico e aplicações comerciais, variando entre "Rôbos Cientistas", filtros anti-spam e sistemas de recomendação.

Todavia, você observará que sua necessidade de conhecer sobre machine learning pois ele é um dos tópicos que você precisa dominar se deseja ser um expert em Ciência de Dados.

O tutorial de hoje irá lhe introduzir ao básico do machine learning com pyhton: E, iremos lhe mostrar passo a passo, como utilizar Python para trabalhar com os conhecidos algoritmos de aprendizado de máquina não supervisionado.

Caso você se interesse ainda mais pelo assunto, há um tutorial que cobre machine learning com R, que você pode encontrar em [Machine Learning com R para Iniciantes](https://www.datacamp.com/community/tutorials/machine-learning-in-r)

### Carregando seu conjunto de dados

Em Data Science, ou Ciência de dados, o primeiro passo antes de qualquer coisa é carregar ou importar seus dados. E, que também é o ponto de partida deste tutorial.

A aprendizagem de máquina, trabalha principalmente com a observação dos dados. Assim, esses dados utilizado podem ser coletados por você ou ainda coletados por intermédido de fontes que disponibilizem bases e/ou amostras de dados. Entretanto, se você ainda não é um pequisador ou está envolvido com pesquisas, você provalmente deixará isso para outro momento.

Se você é novo na área de aprendizado de máquina ou gostaria de inciar sua caminhada em problemas desta área, encontrar amostras de dados pode ser um verdadeiro trabalho de Hércules. Porém, você pode encontrar vários bons conjuntos de dados em repositórios como [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets) ou em sites como [Kaggle](www.kaggle.com). Além disso, você pode encontrar mais bases de dados [nessa lista de recursos do KD Nuggets](http://www.kdnuggets.com/datasets/index.html). 

Por hora, você pode se aquecer, apenas carregando o conjunto de dados `digits` já contida no pacote de aprendizado de máquina do python, o scikit-learn, sem se preocupar em correr atrás de bases de dados.

Curiosidade: Você sabia que o nome desse pacote é uma `caixa de ferramentas` construída ao redor da biblioteca Scipy? Que a propósito, é muito mais que apenas um [kit cientifico](https://scikits.appspot.com/scikits). Esse kit contém módulos especialmentes constuídos à aprendizagem de máquina e mineração de dados, o que além disso dá nome ao pacote. :)

Para carregar os dados, você deve importar o módulo `datasets` do `sklearn`. E, então usar o método `load_digits()` do `datasets` para carregar os dados:

```{python ex="scikit_load", type="sample-code"}
# Import `datasets` from `sklearn`
from sklearn import ________

# Load in the `digits` data
digits = datasets.load_digits()

# Print the `digits` data 
print(______)
```

```{python ex="scikit_load", type="solution"}
# Import `datasets` from `sklearn`
from sklearn import datasets

# Load in the `digits` data
digits = datasets.load_digits()

# Print the `digits` data 
print(digits)
```

```{python ex="scikit_load", type="sct"}
import_msg="Você importou o `datasets` de `sklearn`?"
incorrect_import_msg="Acho que você esqueceu de carregar o módulo `datasets` de `sklearn`!"
not_called_msg="Você utilizou o `datasets.load_digits()` para carregar os dados em `digits`?"
incorrect_msg="Não se esqueça de utilizar `datasets.load_digits()` para carregar os dados do `digits`!"
predef_msg="Você chamou a função `print()`?"
test_import("sklearn.datasets", same_as = True, not_imported_msg = import_msg, incorrect_as_msg = incorrect_import_msg)
test_function("sklearn.datasets.load_digits", not_called_msg = not_called_msg, incorrect_msg = incorrect_msg)
# Test `print()` function
test_function(
    "print",
    not_called_msg=predef_msg,
    incorrect_msg=predef_msg,
    do_eval=False
)
success_msg="Perfeito! Você está pronto para continuar!"
```

Note que o módulo `datasets` contém outros métodos para carregar e buscar conjuntos de dados populares, e, você ainda pode contar com este módulo caso haja a necessidade de gerar dados artifíciais. Além disso, esse conjunto de dados está disponível através do Repositório UCI Machine Learning: onde você pode encontrar esses [dados](http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/).

Caso você queira utilizar os dados diretamente do Repositório UCI, seu código deverá ser similar a este:

```{python ex="pandas_load", type="sample-code"}
# Import the `pandas` library as `pd`
import ______ as __

# Load in the data with `read_csv()`
digits = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)

# Print out `digits`
print(______)
```

```{python ex="pandas_load", type="solution"}
# Import the `pandas` library as `pd`
import pandas as pd

# Load in the data with `read_csv()`
digits = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)

# Print out `digits`
print(digits)
```

```{python ex="pandas_load", type="sct"}
import_msg="Você utilizou algum código para importar `pandas` como `pd`?"  
incorrect_import_msg="Não esqueça de importar a bibloteca 'pandas' como `pd`!"
csv_msg="Você utilizou o método `read_csv()` from pandas para carregar os dados?"
csv_incorrect_msg="Use `read_csv()` da biblioteca pandas para carregar os dados "
predef_msg="Você utilizou a função `print()`?"
# Test import `pandas`
test_import("pandas", same_as = True, not_imported_msg = import_msg, incorrect_as_msg = incorrect_import_msg)
# Test `read_csv()`
test_function("pandas.read_csv", not_called_msg = csv_msg, incorrect_msg = csv_incorrect_msg)
# Test `print()` function
test_function(
    "print",
    not_called_msg=predef_msg,
    incorrect_msg=predef_msg,
    do_eval=False
)
success_msg("Bom trabalho!")
```

Note que caso tenha efetuado o download da base de dados do Repositório UCI Machine Learning, os dados já estão divididos em dois conjuntos um de treinamento e outro de teste, indicado respectivamente pelas arquivos com extensões `.tra` e `.tes`. Então, será necessário fazer o upload de ambos arquivos para seu projeto de machine learning. Vale lembrar que com os comandos acima, você apeas carregou o conjunto de treinamento.

Dica: Se você quiser saber mais sobre como importar dados com a biblioteca Pandas de manipulação de dados em python, seria interessante você conhecer o [Curso de importação de dados em Python](https://www.datacamp.com/courses/importing-data-in-python-part-1) do DataCamp.

### Explorando seus dados

Como tudo começa com seu conjunto de dados, é sempre uma boa ideia olhar a descrição dos dados e ver o que já podemos aprender, se disponível. Porém, quando estamos utilizando a scikit-learn, você não precisa ter essa informação prontamente disponível, entretanto, caso você utilize uma base de dados de fontes como o Repositório UCI Machine Learning, essa informação é suficiente para reunir alguns insights dos seus dados.

Entretanto, os insights provenientes das descrições dos dados não são suficiente às analises que você realizará. Assim, você precisará ter um conhecimento mais profundo sobre seu conjunto de dados.

Contudo, realizar uma analise exploratória dos dados desse conjunto de dados, como o deste tutorial pode parecer um pouco difícil.

Então, por onde podemos começar a explorar os dados do conjunto `digits`?

#### Reunindo informações básicas sobre nossos dados

Imaginemos que você ainda não olhou a pasta de descrição dos dados (ou mesmo gostaria de verificar novamente os dados que você tem em mãos).

Você, muito provavelmente, deverá começar reunindo informações básicas.

Assim, você observará que ao utilizar a função `print` para apresentar os dados contidos em `digits`, após ter o carregado com o auxílio do módulo `datasets` do scikit-learn, que há uma quantidade considerável de informação disponível. Dessa forma, você terá o conhecimento de dos cabeçalhos e descrição dos seus dados. Ressalta que você poderá acessar os dados contidos em `digits` através do atributo `data`. E, ainda acessar os rótulos de dados por intermédio do atributo `target` e a descrição dos dados como atributo `DESC`.

Caso, deseje saber quais os atributos disponíveis para conhecer melhor seus dados basta apenas utilizar o comando `digits.keys()`.

Execute todos os blocos abaixo:
```{python ex="digits", type="pre-exercise-code"}
from sklearn import datasets
digits = datasets.load_digits()
```

```{python ex="digits", type="sample-code"}
# Get the keys of the `digits` data
print(digits.______)

# Print out the data
print(digits.____)

# Print out the target values
print(digits.______)

# Print out the description of the `digits` data
print(digits.DESCR)
```

```{python ex="digits", type="solution"}
# Get the keys of the `digits` data
print(digits.keys())

# Print out the data
print(digits.data)

# Print out the target values
print(digits.target)

# Print out the description of the `digits` data
print(digits.DESCR)
```

```{python ex="digits", type="sct"}
# Test `print` 
test_function(
    "print",
    1,
    not_called_msg="Você descobriu todos os paramêtros disponíveis ao `digits`?",
    incorrect_msg="Não se esqueça de descobrir todas as chaves do `digits`!",
    do_eval=False
)
# Test `print`
test_function(
    "print",
    2,
    not_called_msg="Acho que você esqueceu de imprimir os dados?",
    incorrect_msg="Não se esqueça de visualizar seus dados!",
    do_eval=False
)
# Test `print`
test_function(
    "print",
    3,
    not_called_msg="Você não se esqueceu de verificar os rótulos dos seus dados?",
    incorrect_msg=" Não se esqueça de verificar os rótulos dos seus dados!",
    do_eval=False
)
# Test `print` 
test_function(
    "print",
    4,
    not_called_msg="Você não se esqueceu de verificar a descrição do `digits`?",
    incorrect_msg="Não se esqueça de verificar a descrição do `digits`!",
    do_eval=False
)
success_msg("Fantástico!")
```

O próximo passo que você deve fazer é verificar novamente o tipo dos seus dados.

Caso você tenha utilizado o `read_csv()` para importar seus dados, seus dados estarão contidos em um `DataFrame`. Nesse caso não haverá nenhum componente para descrição dos dados, mas você poderá verificar, por exemplo, os dados contidos no inicio ou final do se conjunto de dados, respectivamente, utilizando os métodos `head()` ou `tail()` da biblioteca Pandas. Em casos como este, verificar a descrição dos dados é sempre algo inteligente à se fazer!

Porém, esse tutorial assume que você que você utilizará os dados do scikit-learn e o tipo de dados da variável `digits` lhe auxiliará caso você não esteja familiarizado com a biblioteca. Observe o retorno do primeiro conjunto de código. Nele, você verá que o `digits` contém um  numpy array!

O que por si só já uma informação muito importante. Mas, como podemos acessar esses arrays?

Na verdade isso é algo extremamente fácil. Basta apenas você utilizar atributos para acessar os arrays que lhe sejam relevantes.

Só não se esqueça que você já viu os atributos disponíveis ao utilizar o comando `digits.keys()`. Por exemplo, você pode utilizar o método `data` para isolar os dados, o `target` para ver os rótulos de dados  e o `DESCR` para a descrição dos dados, ...

Mas, e agora?!

A primera coisa que você precisa saber sobre seu array de dados é seu formato. Ou seja, o número de dimensões e itens contidos no seu array de dados. O formato de um array é especificado por uma tupla de inteiros que específica o tamanho de cada uma de suas dimensões. Em outras palavras, caso você tenha uma array 3D como por exemplo ```y = np.zeros((2, 3, 4))```, o formato do seu array será ```(2,3,4)```.

Agora, é a hora de visualizar o formato dos três arrays que encontramos ( `data`,` target` e `DESCR`).

Para verificar o formato dos arrays, os passos são basicamente os mesmos, diferindo apenas nos atributos utilizados. Assim, vamos tomar por exemplo o atributo `data`, basta isolar o array numpy do `digits` e então utilizar o atributo `shape`, feito isso você saberá o formato do atributo `data`. O mesmo pode ser aplicado no `target` e `DESCR`. Há ainda o atributo `images`, que basicamente são os dados das imagens. Você agora pode realizar o teste.

Observe na declaração abaixo utilzando o atributo `shape` no array:

```{python ex="digits_shape", type="pre-exercise-code"}
from sklearn import datasets
import numpy as np
digits = datasets.load_digits()
```

```{python ex="digits_shape", type="sample-code"}
# Isolate the `digits` data
digits_data = digits.data

# Inspect the shape
print(digits_data.shape)

# Isolate the target values with `target`
digits_target = digits.______

# Inspect the shape
print(digits_target._____)

# Print the number of unique labels
number_digits = len(np.unique(digits.target))

# Isolate the `images`
digits_images = digits.images

# Inspect the shape
print(digits_images.shape)
```

```{python ex="digits_shape", type="solution"}
# Isolate the `digits` data
digits_data = digits.data

# Inspect the shape
print(digits_data.shape)

# Isolate the target values with `target`
digits_target = digits.target

# Inspect the shape
print(digits_target.shape)

# Print the number of unique labels
number_digits = len(np.unique(digits.target))

# Isolate the `images`
digits_images = digits.images

# Inspect the shape
print(digits_images.shape)
```

```{python ex="digits_shape", type="sct"}
msg_data="Você adicionou o atributo `shape` para verificar o número de dimensões e itens do array `digits_data`?"
msg_target="Você adicionou o atributo `shape` para verificar o número de dimensões e itens do array `digits_target`?"
msg_image="Você adicionou o atributo `shape` para verificar o número de dimensões e itens do array `digits_images`?"
# Test object `digits_data`
test_object("digits_data", undefined_msg="Você já definiu o objeto `digits_data`?", incorrect_msg="Você utilizou o atributo `data` para isolar os dados do `digits`?")
# Test object `digits_target`
test_object("digits_target", undefined_msg="Você já definiu o objeto `digits_target`?", incorrect_msg="`target`")
# Test `shape` of `digits_data`
#test function print
test_function(
    "print",
    1,
    not_called_msg="Você já imprimiu o formatos dos dados??",
    incorrect_msg="Não se esqueça de imprimir o formato dos dados!",
    do_eval=False
)
test_object_accessed("digits_data.shape", not_accessed_msg=msg_data)
# Test `print`
test_function(
    "print",
    2,
    not_called_msg="Você já imprimiu o formatos dos rórulos dos dados?",
    incorrect_msg="Não se esqueça de imprimir o formato dos rótulos dos dados!",
    do_eval=False
)
# Test access `shape` of `digits_target`
test_object_accessed("digits_target.shape", not_accessed_msg=msg_target)
# Test object `number_digits`
test_object("number_digits", undefined_msg="Você já definiu o objeto `number_digits`?", incorrect_msg="Você utilizou o `np.unique()` para retornar os valores únicos dos rótulos de dados? Não se esqueça buscar o tamanho desse array utilizando o `len()`!")
# Test object `digits_images`
test_object("digits_images", undefined_msg="Did you define the `digits_images` object?", incorrect_msg="Você utilizaou o atributo `images` para isolar as imagens dos dados do `digits`?")
# Test `shape` of `digits_images`
test_object_accessed("digits_images.shape", not_accessed_msg=msg_image)
# Test `print` 
test_function(
    "print",
    3,
    not_called_msg="Você imprimiu o formato das imagens do `digits`?",
    incorrect_msg="Não se esqueça de imprimir o formato das imagens do `digits`!",
    do_eval=False
)
success_msg("Muito bem!")
```

Recapitulando: ao inspecionar o `digits.data`, você verá que há 1797 amostras e 64 atributos. Assim, você terá 1797 amostras e 1797 valores de rótulos.

Todos os rótulos contém 10 valores únicos, de 0 a 9. Em outras palavras, todos os 1797 rótulos são formados por números entre 0 e 9. Isso significa que os valores que serão reconhecidos por seu modelo conterão números entre 0 e 9.

Por último, você verá que os dados do atributo `images` contém  três dimensões: há 1797 instâncias de tamanho 8x8 pixels. Você pode visualizar que os atributos `images` e `data` se relacionam, o que pode ser observado ao reformatar o `images` em duas dimensões, utilizando o seguinte código: `digits.images.reshape((1797, 64))`.

Todavia, caso você queira ter certeza sobre é melhor utilizar o ```print(np.all(digits.images.reshape((1797,64)) == digits.data))```. Com o método `all()` do numpy, você testará se todos os elementos de um array, de um eixo, são iguais ao valor `True`. Nesse caso, você avaliará se o novo formato do array `images` é igual ao array `digits.data`. Nesse caso você verá que o resultado é `True`.

#### Visualize seus dados com `matplotlib`

Agora, você pode levar a exploração dos seus dados a outro nível, visualizando os dados com que você estará trabalhando. Para tal, você pode utilizar uma das várias bibliotecas para visualização de dados do Python, como a por exemplo a [matplotlib](http://matplotlib.org/):

```
# Import matplotlib
import matplotlib.pyplot as plt

# Figure size (width, height) in inches
fig = plt.figure(figsize=(6, 6))

# Adjust the subplots 
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# For each of the 64 images
for i in range(64):
    # Initialize the subplots: add a subplot in the grid of 8 by 8, at the i+1-th position
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
    # Display an image at the i-th position
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    # label the image with the target value
    ax.text(0, 7, str(digits.target[i]))

# Show the plot
plt.show()
```

O código aparenta ser meio complicado e pode ser meio assustador. Entretanto, ele é bastante simples e fácil de se compreender uma vez que você o divida em partes:
The code chunk seems quite lenghty at first sight and this might be overwhelming. But, what happens in the code chunk above is actually pretty easy once you break it down into parts:

- Você importou a biblioiteca `matplotlib`;
- A seguir, você configurou uma figura com 6 inches de largura e altura. Esse é o fundo branco, onde, suas imagens serão apresentadas;
- Então, você configurou um novo nível, ajustando as margens: direita e esquerda, superior e inferior. E, por fim a altura e largura. Ressalta que esses ajustes são para o layout.
- Após isso, você iniciou o preenchimento da figura com o auxílio da estrutura de repetição `for`;
- Você iniciou os subplotss, um por um, adicionando a cada uma das posições no grids uma imagem de tamanho `8`x`8`.
- Você apresentou a cada interação uma das imagens em suas posições no grid. Utilizando o mapa de cores, você usou cores binárias, neste caso as cores preta, branca e cinza. Utilizou o método de interpolação `nearest`, o que significa que seus dados foram interpolados de maneira não muito sauve. Caso você queira saber mais sobre o efeito de diferentes métodos de interpolação acesse esse [link](http://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.html);
- A cereja do bolo é adição de texto em seu subplots. O rótulos dos dados serão impressos nas coordenadas (0,7) de cada subplot, ou seja, irão se apresentadas no canto inferior-esquerdo de cada um dos subplots;
- E, é claro não se esqueça de imprimir sua visualização como auxílio do comando `plt.show()`!

Ao final, você terá como resultado a seguinte visualização:
[INSERT VISUALIZATION/PLOT1]

Além disso, você ainda pode visualizar os rótulos com as imagens, apenas utilizando esse código:

```
# Import matplotlib
import matplotlib.pyplot as plt 

# Join the images and target labels in a list
images_and_labels = list(zip(digits.images, digits.target))

# for every element in the list
for index, (image, label) in enumerate(images_and_labels[:8]):
    # initialize a subplot of 2X4 at the i+1-th position
    plt.subplot(2, 4, index + 1)
    # Don't plot any axes
    plt.axis('off')
    # Display images in all subplots 
    plt.imshow(image, cmap=plt.cm.gray_r,interpolation='nearest')
    # Add a title to each subplot
    plt.title('Training: ' + str(label))

# Show the plot
plt.show()
```
O que resultará nesta visualização:
[INSERT PICTURE HERE/PLOT2]


Observe que nesse caso, após você ter importado a biblioteca `matplotlib`, é necessário que você concatene ambos arrays numpy e os salve na variável `images_and_labels`. Assim, você verá que essa lista conterá todos suplementos da instância de `digits,images` e correspondentes aos valores `digits.target`.

Então, você diz aos primeiros oito elementos de `images_and_labels` -lembre-se que o índice começa em 0! -, inicializem em um subplot de 2x4 em cada posição. Assim, você transforma o gráfico dos eixos e exibe imagens em todas as subplots com um mapa de cores `plt.cm.gray_r` (que retorna todas as cores cinzas), utilizando método de interpolação `nearest`. E, por fim, inclui um título para cada subplot, e apresenta o resultado.

Nem foi tão difícil, né?!

E, agora você tem uma boa ideia dos dados com os quais você está trabalhando!

#### Visualizando seus dados: Analise do Componente Principal (ACP)

Mas, o que podemos dizer sobre os dados? E, há outra forma para visualizar os dados?

Como a conjunto de dados `digits` contem 64 características, isso pode ser uma desafio. Como você pode imaginar é muito difícil de se entender a estrutura e manter um overview do conjunto de dados `digits`. Assim, pode-se dizer que estamos a trabalhar com conjuntos de dados multidimensional.

A multidimensionalidade dos dados é um resultado direto da tentativa de se descrever objetos por meio de uma coleção de caracteristicas. Assim, podemos encontrar outros exemplos de dados com essas características em dados financeiros, dados climáticos, neuroimagens, entre outros.

Entretanto, como você já deve ter imaginado, trabalhar com essa diversidade de dimenões não é algo fácil. Portanto, em algums casos, essa multidimensionalidade dos dados pode ser um problema, visto que, seus algoritmos de machine learning terão de levar em conta as diferentes facetas dos dados. Assim, esse casos, são conhecidos como a maldição da dimensionalidade. Por conta da grande quantidade de dimensões dos dados, seus dados podem estar virtualmente distantes uns dos outros, fazendo com o que essa distância entre eles não seja informátiva.

Porém, não se preocupe com a maldição não se trata apenas da contagem da quantidade das facetas dos dados. Há outros casos, em que a o tamanho da dimensionalidade pode ser menor que o número de características, casos assim algumas das facetas são, simplesmente, irrelevantes.

Assim sendo, você pode verificar que dados com apenas duas ou três dimensões são mais facéis de se analisar, e, consequentemente visualizar.

Portanto tudo isso explica como você irá visualizar os dados com auxílio de técnicas de reduções de dimensionalidade, conhecido como Analise do Componente Principal (ACP). A ideia do ACP é encontra uma combinação linear de duas variáveis que contenham a maioria da informação. Essa nova variável ou "componente principal", poderá substituir as duas variáveis originais.

Simplificando, trata-se de um método transformação linear para reduzir as direções (componentes principais), a fim de maximizar a variância dos dados. Não se esqueça, que essa variância indica quando distântes os pontos dos conjuntos de dados estão entre si. Caso queira saber mais, visite [esse link](http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca). 

Você pode facilmente aplicar o ACP em seus dados com auxílio do scikit-learn:

```{python ex="pca", type="pre-exercise-code"}
from sklearn import datasets
digits = datasets.load_digits()
from sklearn.decomposition import RandomizedPCA
from sklearn.decomposition import PCA
import numpy as np
```

```{python ex="pca", type="sample-code"}
# Create a Randomized PCA model that takes two components
randomized_pca = RandomizedPCA(n_components=2)

# Fit and transform the data to the model
reduced_data_rpca = randomized_pca.fit_transform(digits.data)

# Create a regular PCA model 
pca = PCA(n_components=2)

# Fit and transform the data to the model
reduced_data_pca = pca.fit_transform(digits.data)

# Inspect the shape
reduced_data_pca.shape

# Print out the data
print(reduced_data_rpca)
print(reduced_data_pca)
```

```{python ex="pca", type="solution"}
# Create a Randomized PCA model that takes two components
randomized_pca = RandomizedPCA(n_components=2)

# Fit and transform the data to the model
reduced_data_rpca = randomized_pca.fit_transform(digits.data)

# Create a regular PCA model 
pca = PCA(n_components=2)

# Fit and transform the data to the model
reduced_data_pca = pca.fit_transform(digits.data)

# Inspect the shape
reduced_data_pca.shape

# Print out the data
print(reduced_data_rpca)
print(reduced_data_pca)
```

```{python ex="pca", type="sct"}
test_object("randomized_pca", do_eval=False)
test_object("reduced_data_rpca", do_eval=False)
test_object("pca", do_eval=False)
test_object("reduced_data_pca", do_eval=False)
predef_msg="Did you inspect the shape of `reduced_data_pca`?"
test_object_accessed("reduced_data_pca.shape", not_accessed_msg=predef_msg)
# Test `print` 
test_function(
    "print",
    1,
    not_called_msg="Você imprimiu os dados do `reduced_data_rpca`?",
    incorrect_msg="Não se esqueça de imprimir os dados do `reduced_data_rpca`!",
    do_eval=False
)
test_function(
    "print",
    2,
    not_called_msg="Você imprimiu os dados do `reduced_data_pca`?",
    incorrect_msg="Não se esqueça de imprimir os dados do `reduced_data_pca`!",
    do_eval=False
)
success_msg("Maravilha!")
```

**Dica**: Você utilizou o `RandomizedPCA()`, neste caso, pois sua performance é melhor quando há um alto número de dimensões. Assim, você pode substituir esse metódo randômico do módelo ACP ou utilizar um outro método regular do módelo para ver a diferença entre eles.

Veja, ainda, como você diz explicitamente ao modelo para manter apenas dois componentes. Isso fará com que você a certeza de que seus dados agora são bidimensionais. Note, ainda, como você não passou a classe target com os rótulos para a transformação ACP, isso pois, caso queira investigar se a ACP revela uma distribuição distintas de rótulos, e, se você consegue separar as instâncias entre si.

Você pode construir um gráfico de dispersão para visualizar os dados:

```
colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']
for i in range(len(colors)):
    x = reduced_data_rpca[:, 0][digits.target == i]
    y = reduced_data_rpca[:, 1][digits.target == i]
    plt.scatter(x, y, c=colors[i])
plt.legend(digits.target_names, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title("PCA Scatter Plot")
plt.show()
```

Sua visualização será semelhante a essa:

[INSERT PICTURE HERE/PLOT3]

Novamente, você utilizou o `matplotlib` para visualizar os dados. Essa biblioteca é excelente para uma visualização rápida dos dados com os quais você está trabalhando, mas você deve sempre pensar em algo mais elegante, caso você esteja a trabalhar em seu portifólio em Ciência de dados.

Ressalta-se que o último método utilizado para plotar a visualização (`plt.show()`) não será necessário caso você esteja trabalhando no Jupyter Notebook, visto que você desejará as colocar a visualização junto ao seu código. Então, sempre que houver dúvidas, você poderá consultar nosso [Guia Definitido do Jupyter Notebook](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook). 

O código acima, realiza os seguintes passos:

1. Primeiro, coloca todas as cores que serão utilizadas em uma lista;
Observe que sua lista de cores, possui a mesma quantidade de rótulos que você possue. Assim, você terá certeza que os pontos em seus dados estão sendo coloridos de acordo com seus rórulos. Assim, você define suas cores num intervalo entre 0 e 10. Tenha sempre em mente que esse intervalo não é inclusivo.
2. Define as coordenadas `x` e  `y`;
Assim, você pegará a primeira ou segunda coluna de `reduced_data_rpca`, e selecionará apenas os pontos nos quais os rótulos iguais ao index que você definiu. Isso significa quem em um primeiro momento, você irá considerar os com rótulo `0`, `1` e assim sucessivamente.
3. Constroe seu gráfico de dispersão.
Preenchendo as coordenadas `x` e `y`, e, utilzando as cores que você definiu. No primeiro momento, todos marcadores terão a cor `preta`, então `azul`, e o processo continuará até os marcadores estarem todos marcados corretamente.
4. Adiciona uma legenda ao gráfico. Utilizando para isso o método `target_names` para conseguir os rótulos corretos aos marcadores dos dados;
5. Adiciona os rótulos significativos aos eixos `x` e `y`;
6. Apresenta o resultado obtido.

### Where To Go Now?

Now that you have even more information about your data and you have a visualization ready, it does seem a bit like the data points sort of group together, but you also see there is quite some overlap. 

This might be interesting to investigate further. 

Do you think that, in a case where you knew that there are 10 possible digits labels to assign to the data points, but you have no access to the labels, the observations would group or "cluster" together by some criterion in such a way that you could infer the lables?

Now this is a research question!

In general, when you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant to your data set. In other words, you think about what your data set might teach you or what you think you can learn from your data. 

From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.

**Tip:** the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. The same also holds for finding the appropriate machine algorithm. 

However, when you're first getting started with scikit-learn, you'll see that the package is pretty vast and you might still want additional help when you're doing the assessment for your data set. That's why [this scikit-learn machine learning map](http://scikit-learn.org/stable/tutorial/machine_learning_map/) will come in handy. 

Note that this map does require you to have some knowledge about the machine learning algorithms that are included in the scikit-learn package. This, by the way, also holds some truth for taking this next step in your machine learning project: if you have no idea what is possible with machine learning, it will be very hard to decide on what your use case will be for the data.

As your use case was one for clustering, you can follow the path on the machine learning map towards "KMeans". You'll see the use case that you have just thought about requires you to have more than 50 samples ("check!"), to have labeled data ("check!"), to know the number of categories that you want to predict ("check!") and to have less than 10K samples ("check!").

But what exactly is the K-Means algorithm?

It is one of the simplest and widely used unsupervised learning algorithms to solve clustering problems. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters that you have set before you run the algorithm. This number of clusters is called `k` and you select this number at random.

Then, the k-means algorithm will find the nearest cluster center for each data point and assign the data point closest to that cluster. 

Once all data points have been assigned to clusters, the cluster centers will be recomputed. In other words, new cluster centers will emerge from the average of the values of the cluster data points. This process is repeated until most data points stick to the same cluster. The cluster membership should stabilize. 

You can already see that, because the k-means algorithm works the way it does, the initial set of cluster centers that you give up can have a big effect on the clusters that are eventually found. You can, of course, deal with this effect, as you will see further on.

However, before you can go into making a model for your data, you should definitely take a look into preparing your data for this purpose. 

### Preprocessing Your Data

As you have read in the previous section, before modeling your data, you'll do well by preparing it first. This preparation step is called "preprocessing".

#### Data Normalization

The first thing that we're going to do is preprocessing the data. You can standardize the `digits` data by, for example, making use of the `scale()` method: 

```{python ex="normalization", type="pre-exercise-code"}
from sklearn import datasets
digits = datasets.load_digits()
```

```{python ex="normalization", type="sample-code"}
# Import
from sklearn.preprocessing import scale

# Apply `scale()` to the `digits` data
data = _____(digits.data)
```

```{python ex="normalization", type="solution"}
# Import
from sklearn.preprocessing import scale

# Apply `scale()` to the `digits` data
data = scale(digits.data)
```

```{python ex="normalization", type="sct"}
test_function(
    "sklearn.preprocessing.scale",
    not_called_msg="Did you standardize the `digits` data?",
    incorrect_msg="Don't forget to standardize the `digits` data with `scale()`!",
    do_eval=False
)
success_msg("Awesome!")
```


By scaling the data, you shift the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance).

#### Splitting Your Data Into Training And Test Sets

In order to assess your model's performance later, you will also need to divide the data set into two parts: a training set and a test set. The first is used to train the system, while the second is used to evaluate the learned or trained system. 

In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

You will try to do this also here. You see in the code chunk below that this 'traditional' splitting choice is respected: in the arguments of the `train_test_split()` method, you clearly see that the `test_size` is set to `0.25`. 

You'll also note that the argument `random_state` has the value `42` assigned to it. With this argument, you can guarantee that your split will always be the same. That is particularly handy if you want reproducible results.

```{python ex="train_test_split", type="pre-exercise-code"}
from sklearn import datasets
digits = datasets.load_digits()
from sklearn.preprocessing import scale
data = scale(digits.data)
```

```{python ex="train_test_split", type="sample-code"}
# Import `train_test_split`
from sklearn.cross_validation import ________________

# Split the `digits` data into training and test sets
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
```

```{python ex="train_test_split", type="solution"}
# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the `digits` data into training and test sets
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
```

```{python ex="train_test_split", type="sct"}
import_msg="Did you import `train_test_split` from `sklearn.cross_validation`?"
predef_msg="Don't forget to fill in `train_test_split`!"
test_import("sklearn.cross_validation.train_test_split", same_as = True, not_imported_msg = import_msg, incorrect_as_msg = predef_msg)
test_object("X_train", do_eval=False,  undefined_msg="Did you leave out `X_train` or any of the other variables?")
test_object("X_test", do_eval=False, undefined_msg="Did you define `X_test`?")
test_object("y_train", do_eval=False, undefined_msg="Did you define `y_train`?")
test_object("y_test", do_eval=False, undefined_msg="Did you define `y_test`?")
test_object("images_train", do_eval=False, undefined_msg="Did you define `images_train`?")
test_object("images_test", do_eval=False, undefined_msg="Did you define `images_test`?")
success_msg("Great job!")
```


After you have split up your data set into train and test sets, you can quickly inspect the numbers before you go and model the data:

```{python ex="inspect", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import scale
import numpy as np
digits = datasets.load_digits()
data = scale(digits.data)
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
```

```{python ex="inspect", type="sample-code"}
# Number of training features
n_samples, n_features = X_train.shape

# Print out `n_samples`
print(_________)

# Print out `n_features`
print(__________)

# Number of Training labels
n_digits = len(np.unique(y_train))

# Inspect `y_train`
print(len(_______))
```

```{python ex="inspect", type="solution"}
# Number of training features
n_samples, n_features = X_train.shape

# Print out `n_samples`
print(n_samples)

# Print out `n_features`
print(n_features)

# Number of Training labels
n_digits = len(np.unique(y_train))

# Inspect `y_train`
print(len(y_train))
```

```{python ex="inspect", type="sct"}
test_object("n_samples", undefined_msg="did you leave out `n_samples` or `n_features`?")
test_object("n_features")
test_function(
    "print",
    1,
    not_called_msg="Did you print out the number of samples of the `digits` training data?",
    incorrect_msg="Don't forget to print out the number of samples!",
    do_eval=False
)
test_function(
    "print",
    2,
    not_called_msg="Did you print out the number of features of the `digits` training data?",
    incorrect_msg="Don't forget to print out the number of features!",
    do_eval=False
)
test_object("n_digits", incorrect_msg="did you define `n_digits` correctly?")
test_function(
    "print",
    3,
    not_called_msg="Did you print out the number of training labels for the `digits` data?",
    incorrect_msg="Don't forget to print out the number of training labels with `len(y_train)`!",
    do_eval=False
)
success_msg("Well done!")
```


You'll see that the training set `X_train` now contains 1347 samples, which is exactly 2/3d of the samples that the original data set contained, and 64 features, which hasn't changed. The `y_train` training set also contains 2/3d of the labels of the original data set. This means that the test sets `X_train` and `y_train` contain 450 samples. 

### Clustering The `digits` Data 

After all these preparation steps, you have made sure that all your known (training) data is stored. No actual model or learning was performed up until this moment. 

Now, it's finally time to find those clusters of your training set. Use `KMeans()` from the `cluster` module to set up your model. You'll see that there are three arguments that are passed to this method: `init`, `n_clusters` and the `random_state`. 

You might still remember this last argument from before when you split the data into training and test sets. This argument basically guaranteed that you got reproducible results. 

```{python ex="kmeans_model", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import scale
import numpy as np
digits = datasets.load_digits()
data = scale(digits.data)
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
```

```{python ex="kmeans_model", type="sample-code"}
# Import the `cluster` module
from sklearn import ________

# Create the KMeans model
clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)

# Fit the training data `X_train`to the model
clf.fit(________)
```

```{python ex="kmeans_model", type="solution"}
# Import the `cluster` module
from sklearn import cluster

# Create the KMeans model
clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)

# Fit the training data to the model
clf.fit(X_train)
```

```{python ex="kmeans_model", type="sct"}
import_msg="Did you import `cluster` from `sklearn`?"
predef_msg="Don't forget to import `cluster from `sklearn`!"
test_import("sklearn.cluster", same_as = True, not_imported_msg = import_msg, incorrect_as_msg = predef_msg)
test_object("clf", do_eval=False, incorrect_msg="did create the KMeans model correctly?")
test_function("clf.fit", do_eval=False)
success_msg("Woohoo!")
```


The `init` indicates the method for initialization and even though it defaults to ‘k-means++’, you see it explicitly coming back in the code. That means that you can leave it out if you want. Try it out in the DataCamp Light chunk above!

Next, you also see that the `n_clusters` argument is set to `10`. This number not only indicates the number of clusters or groups you want your data to form, but also the number of centroids to generate. Remember that a cluster centroid is the middle of a cluster. 

Do you also still remember how the previous section described this as one of the possible disadvantages of the K-Means algorithm? 

That is, that the initial set of cluster centers that you give up can have a big effect on the clusters that are eventually found? 

Usually, you try to deal with this effect by trying several initial sets in multiple runs and by selecting the set of clusters with the minimum sum of the squared errors (SSE). In other words, you want to minimize the distance of each point in the cluster to the mean or centroid of that cluster. 

By adding the `n-init` argument to `KMeans()`, you can determine how many different centroid configurations the algorithm will try. 

**Note** again that you don't want to insert the test labels when you fit the model to your data: these will be used to see if your model is good at predicting the actual classes of your instances!

You can also visualize the images that make up the cluster centers as follows:

```
# Import matplotlib
import matplotlib.pyplot as plt

# Figure size in inches
fig = plt.figure(figsize=(8, 3))

# Add title
fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')

# For all labels (0-9)
for i in range(10):
    # Initialize subplots in a grid of 2X5, at i+1th position
    ax = fig.add_subplot(2, 5, 1 + i)
    # Display images
    ax.imshow(clf.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)
    # Don't show the axes
    plt.axis('off')

# Show the plot
plt.show()
```

[INSERT IMAGES HERE/PLOT4]

The next step is to predict the labels of the test set:

```{python ex="predict", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import scale
from sklearn import cluster
digits = datasets.load_digits()
data = scale(digits.data)
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)
clf.fit(X_train)
```

```{python ex="predict", type="sample-code"}
# Predict the labels for `X_test`
y_pred=clf.predict(X_test)

# Print out the first 100 instances of `y_pred`
print(y_pred[:100])

# Print out the first 100 instances of `y_test`
print(y_test[:100])

# Study the shape of the cluster centers
clf.cluster_centers_._____
```

```{python ex="predict", type="solution"}
# Predict the labels for `X_test`
y_pred=clf.predict(X_test)

# Print out the first 100 instances of `y_pred`
print(y_pred[:100])

# Print out the first 100 instances of `y_test`
print(y_test[:100])

# Study the shape of the cluster centers
clf.cluster_centers_.shape
```

```{python ex="predict", type="sct"}
test_object("y_pred")
test_function(
    "print",
    1,
    not_called_msg="Did you print out the first 100 instances of `y_pred`?",
    incorrect_msg="Don't forget to print out the first 100 instances of `y_pred`!",
    do_eval=False
)
test_function(
    "print",
    2,
    not_called_msg="Did you print out the first 100 instances of `y_test`?",
    incorrect_msg="Don't forget to print out the first 100 instances of `y_test`!",
    do_eval=False
)
msg_data="Did you fill in `shape` to print out the shape of the cluster centers?"
test_object_accessed("clf.cluster_centers_.shape", not_accessed_msg=msg_data)
success_msg="Awesome!"
```


In the code chunk above, you predict the values for the test set, which contains 450 samples. You store the result in `y_pred`. You also print out the first 100 instances of `y_pred` and `y_test` and you immediately see some results. 

In addition, you can study the shape of the cluster centers: you immediately see that there are 10 clusters with each 64 features. 

But this doesn't tell you much because we set the number of clusters to 10 and you already knew that there were 64 features. 

Maybe a visualization would be more helpful. 

Let's visualize the predicted labels:

```
# Import `Isomap()`
from sklearn.manifold import Isomap

# Create an isomap and fit the `digits` data to it
X_iso = Isomap(n_neighbors=10).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
clusters = clf.fit_predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust layout
fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=clusters)
ax[0].set_title('Predicted Training Labels')
ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)
ax[1].set_title('Actual Training Labels')

# Show the plots
plt.show()
```

You use `Isomap()` as a way to reduce the dimensions of your high-dimensional data set `digits`. The difference with the PCA method is that the Isomap is a non-linear reduction method.

[INSERT IMAGE HERE/PLOT5]

**Tip**: run the code from above again, but use the PCA reduction method instead of the Isomap to study the effect of reduction methods yourself. 

You will find the solution here:

```
# Import `PCA()`
from sklearn.decomposition import PCA

# Model and fit the `digits` data to the PCA model
X_pca = PCA(n_components=2).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
clusters = clf.fit_predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust layout
fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)
ax[0].set_title('Predicted Training Labels')
ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)
ax[1].set_title('Actual Training Labels')

# Show the plots
plt.show()
```

[INSERT IMAGE HERE/PLOT6]

At first sight, the visualization doesn't seem to indicate that the model works well.

But this needs some further investigation.

### Evaluation of Your Clustering Model

And this need for further investigation brings you to the next essential step in machine learning, which is the evaluation of your model's performance. In other words, you want to analyze the degree of correctness of the model's predictions.

Let's print out a confusion matrix:

```{python ex="confusion", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import scale
from sklearn import cluster
digits = datasets.load_digits()
data = scale(digits.data)
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)
clf.fit(X_train)
y_pred=clf.predict(X_test)
```

```{python ex="confusion", type="sample-code"}
# Import `metrics` from `sklearn`
from sklearn import _______

# Print out the confusion matrix with `confusion_matrix()`
print(metrics.confusion_matrix(y_test, y_pred))
```

```{python ex="confusion", type="solution"}
# Import `metrics` from `sklearn`
from sklearn import metrics

# Print out the confusion matrix with `confusion_matrix()`
print(metrics.confusion_matrix(y_test, y_pred))
```

```{python ex="confusion", type="sct"}
test_import("sklearn.metrics", same_as = True, not_imported_msg = "Did you import `metrics` from `sklearn`?", incorrect_as_msg = "Don't forget to import `metrics` from `sklearn`!")
test_function(
    "print",
    not_called_msg="Did you print out the confusion matrix?",
    incorrect_msg="Don't forget to print out the confusion matrix!",
    do_eval=False
)
success_msg="Well done! Now, what do the results tell you?"
```


At first sight, the results seem to confirm our first thoughts that you gathered from the visualizations. Only the digit `5` was classified correctly in 41 cases. Also, the digit `8` was classified correctly in 11 instances. But this is not really a success. 

You might need to know a bit more about the results than just the confusion matrix. 

Let's try to figure out something more about the quality of the clusters by applying different cluster quality metrics. That way, you can judge the goodness of fit of the cluster labels to the correct labels.

```{python ex="clustering_performance", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import scale
from sklearn import cluster
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score
digits = datasets.load_digits()
data = scale(digits.data)
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)
clf.fit(X_train)
y_pred=clf.predict(X_test)
```

```{python ex="clustering_performance", type="sample-code"}
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score
print('% 9s' % 'inertia    homo   compl  v-meas     ARI AMI  silhouette')
print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'
          %(clf.inertia_,
      homogeneity_score(y_test, y_pred),
      completeness_score(y_test, y_pred),
      v_measure_score(y_test, y_pred),
      adjusted_rand_score(y_test, y_pred),
      adjusted_mutual_info_score(y_test, y_pred),
      silhouette_score(X_test, y_pred, metric='euclidean')))
```


You'll see that there are quite some metrics to consider:

* The homogeneity score tells you to what extent all of the clusters contain only data points which are members of a single class.
* The completeness score measures the extent to which all of the data points that are members of a given class are also elements of the same cluster.
* The V-measure score is the harmonic mean between homogeneity and completeness.
* The adjusted Rand score measures the similarity between two clusterings and considers all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.
* The Adjusted Mutual Info (AMI) score is used to compare clusters. It measures the similarity between the data points that are in the clusterings, accounting for chance groupings and takes a maximum value of 1 when clusterings are equivalent.
* The silhouette score measures how similar an object is to its own cluster compared to other clusters. The silhouette scores ranges from -1 to 1, where a higher value indicates that the object is better matched to its own cluster and worse mached to neighboring clusters. If many points have a high value, the clusteirng configuration is good. 


You clearly see that these scores aren't fantastic: for example, you see that the value for the silhouette score is close to 0, which indicates that the sample is on or very close to the decision boundary between two neighboring clusters. This could indicate that the samples could have been assigned to the wrong cluster. 

Also the ARI measure seems to indicate that not all data points in a given cluster are similar and the completeness score tells you that there are definitely data points that weren't put in the right cluster. 

Clearly, you should consider another estimator to predict the labels for the `digits` data.

### Trying Out Another Model: Support Vector Machines (SVM)

You saw before that, when you recap all of the information that you have gathered out of the data exploration, that you could build a machine learning model to predict which digit belongs to an image. Clustering assumes that you don't know the labels.

Let's assume that you depart from the case where the `digits` data and the labels for the data are known. That means that you could also classify the instances while knowing the labels. This is a classification task.

If you follow the scikit-learn machine learning map, you'll see that the first model that you meet is the linear SVC. Let's apply this now to the `digits` data:

```{python ex="svm", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.preprocessing import scale
from sklearn import cluster
digits = datasets.load_digits()
data = scale(digits.data)
```

```{python ex="svm", type="sample-code"}
# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the data into training and test sets 
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)

# Import the `svm` model
from sklearn import svm

# Create the SVC model 
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')

# Fit the data to the SVC model
svc_model.fit(X_train, y_train)
```

```{python ex="svm", type="solution"}
# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the data into training and test sets 
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)

# Import the `svm` model
from sklearn import svm

# Create the SVC model 
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')

# Fit the data to the SVC model
svc_model.fit(X_train, y_train)
```

```{python ex="svm", type="sct"}
test_import("sklearn.cross_validation.train_test_split", same_as = True, not_imported_msg = "Did you import `train_test_split` from `sklearn.cross_validation`?", incorrect_as_msg = "Don't forget to import `train_test_split` from `sklearn.cross_validation`!")
test_object("X_train", do_eval=False, undefined_msg="did you define `X_train`?")
test_object("X_test", do_eval=False, undefined_msg="did you define `X_test`?")
test_object("y_train", do_eval=False, undefined_msg="did you define `y_train`?")
test_object("y_test", do_eval=False, undefined_msg="did you define `y_test`?")
test_object("images_train", do_eval=False, undefined_msg="did you define `images_train`?")
test_object("images_test", do_eval=False, undefined_msg="did you define `images_test`?")
test_import("sklearn.svm", same_as = True, not_imported_msg = "Did you import `svm` from `sklearn`?", incorrect_as_msg = "Don't forget to import `svm` from `sklearn`!")
test_object("svc_model", do_eval=False)
test_function("svc_model.fit", do_eval=False)
success_msg="Great job!"
```


Note that in this example we set the value of `gamma` manually. It is possible to automatically find good values for the parameters by using tools such as grid search and cross validation. However, this will not be the focus for this tutorial. 

If you would have used grid search to adjust your parameters, you would have done something like the following:

```{python ex="grid_search", type="pre-exercise-code"}
from sklearn import svm
from sklearn import datasets
from sklearn.cross_validation import train_test_split
digits = datasets.load_digits()
```

```{python ex="grid_search", type="sample-code"}
# Split the `digits` data into two equal sets
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.5, random_state=0)

# Import GridSearchCV
from sklearn.grid_search import GridSearchCV

# Set the parameter candidates
parameter_candidates = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]

# Create a classifier with the parameter candidates
clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)

# Train the classifier on training data
clf.fit(X_train, y_train)

# Print out the results 
print('Best score for training data:', clf.best_score_)
print('Best `C`:',clf.best_estimator_.C)
print('Best kernel:',clf.best_estimator_.kernel)
print('Best `gamma`:',clf.best_estimator_.gamma)
```


Next, you use the classifier with the classifier and parameter candidates that you have just created to apply it to the second part of your data set. Next, you also train a new classifier using the best parameters found by the grid search. You score the result to see if the best parameters that were found in the grid search are actually working. 

```{python ex="fit_grid_search", type="pre-exercise-code"}
from sklearn import svm
from sklearn import datasets
from sklearn.cross_validation import train_test_split
digits = datasets.load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.5, random_state=0)
from sklearn.grid_search import GridSearchCV
parameter_candidates = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]
clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)
clf.fit(X_train, y_train)
```

```{python ex="fit_grid_search", type="sample-code"}
# Apply the classifier to the test data, and view the accuracy score
clf.score(X_test, y_test)  

# Train and score a new classifier with the grid search parameters
svm.SVC(C=10, kernel='rbf', gamma=0.001).fit(X_train, y_train).score(X_test, y_test)
```


The parameters indeed work well!

Now what does this new knowledge tell you about the SVC classifier that you had modeled before you had done the grid search?

Let's back up to the model that you had made before.

You see that in the SVM classifier, the penalty parameter `C` of the error term is specified at `100.`. Lastly, you see that the kernel has been explicitly specified as a `linear` one. The `kernel `argument specifies the kernel type that you're going to use in the algorithm and by default, this is `rbf`. In other cases, you can specify others such as `linear`, `poly`, ... 

But what is a kernel exactly?

A kernel is a similarity function, which is used to compute similarity between the training data points. When you provide a kernel to a machine learning algorithm, together with the training data and the labels, you will get a classifier, as is the case here. You will have trained a model that assigns new unseen objects into a particular category. For the SVM, you will typicall try to linearly divide your data points.  

However, the grid search tells you that an `rbf` kernel would've worked better. The penalty parameter and the gamma were specified correctly. 

**Tip:** try out the classifier with an `rbf` kernel.

For now, let's just say you just continue with a linear kernel and predict the values for the test set:

```{python ex="svm_predict", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.preprocessing import scale
from sklearn import cluster
digits = datasets.load_digits()
data = scale(digits.data)
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)
from sklearn import svm
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')
svc_model.fit(X_train, y_train)
```

```{python ex="svm_predict", type="sample-code"}
# Predict the label of `X_test`
print(svc_model.predict(______))

# Print `y_test` to check the results
print(______)
```

```{python ex="svm_predict", type="solution"}
# Predict the label of `X_test`
print(svc_model.predict(X_test))

# Print `y_test` to check the results
print(y_test)
```

```{python ex="svm_predict", type="sct"}
test_function(
    "print",
    1,
    not_called_msg="Did you print out the predicted labels of `X_test`?",
    incorrect_msg="Don't forget to print out the predicted labels of `X_test`!",
    do_eval=False
)
test_function(
    "print",
    2,
    not_called_msg="Did you print out the true labels of `y_test`?",
    incorrect_msg="Don't forget to revealing the true labels by printing out `y_test`!",
    do_eval=False
)
success_msg("Well done!")
```


You can also visualize the images and their predicted labels:

```
# Import matplotlib
import matplotlib.pyplot as plt

# Assign the predicted values to `predicted`
predicted = svc_model.predict(X_test)

# Zip together the `images_test` and `predicted` values in `images_and_predictions`
images_and_predictions = list(zip(images_test, predicted))

# For the first 4 elements in `images_and_predictions`
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    # Initialize subplots in a grid of 1 by 4 at positions i+1
    plt.subplot(1, 4, index + 1)
    # Don't show axes
    plt.axis('off')
    # Display images in all subplots in the grid
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    # Add a title to the plot
    plt.title('Predicted: ' + str(prediction))

# Show the plot
plt.show()
```

This plot is very similar to the plot that you made when you were exploring the data:

[INSERT IMAGE/PLOT7]

Only this time, you zip together the images and the predicted values and you only take the first 4 elements of `images_and_predictions`.  

But now the biggest question: how does this model perform?

```{python ex="svc_performance", type="pre-exercise-code"}
from sklearn import datasets
from sklearn.preprocessing import scale
from sklearn import cluster
digits = datasets.load_digits()
data = scale(digits.data)
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)
from sklearn import svm
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')
svc_model.fit(X_train, y_train)
predicted = svc_model.predict(X_test)
```

```{python ex="svc_performance", type="sample-code"}
# Import `metrics`
from sklearn import metrics

# Print the classification report of `y_test` and `predicted`
print(metrics.classification_report(______, _________))

# Print the confusion matrix of `y_test` and `predicted`
print(metrics.confusion_matrix(______, _________))
```

```{python ex="svc_performance", type="solution"}
# Import `metrics`
from sklearn import metrics

# Print the classification report of `y_test` and `predicted`
print(metrics.classification_report(y_test, predicted))

# Print the confusion matrix
print(metrics.confusion_matrix(y_test, predicted))
```

```{python ex="svc_performance", type="sct"}
test_import("sklearn.metrics", same_as = True, not_imported_msg = "Did you import `metrics` from `sklearn`?", incorrect_as_msg = "Don't forget to import `metrics` from `sklearn`!")
not_called_msg="Did you fill in `y_test` and `predicted`?"
incorrect_msg="Don't forget to fill in `y_test` as the first argument, `predicted` as the second argument!"
test_function("print", 1, do_eval=False, not_called_msg = not_called_msg, incorrect_msg = incorrect_msg)
test_function("print", 2, do_eval=False, not_called_msg = not_called_msg, incorrect_msg = incorrect_msg)
success_msg="Well done! Now, check the results of the confusion matrix. Does this model perform better?"
```


You clearly see that this model performs a whole lot better than the clustering model that you used earlier.

You can also see it when you visualize the predicted and the actual labels with the help of `Isomap()`:

```
# Import `Isomap()`
from sklearn.manifold import Isomap

# Create an isomap and fit the `digits` data to it
X_iso = Isomap(n_neighbors=10).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
predicted = svc_model.predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust the layout
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=predicted)
ax[0].set_title('Predicted labels')
ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)
ax[1].set_title('Actual Labels')


# Add title
fig.suptitle('Predicted versus actual labels', fontsize=14, fontweight='bold')

# Show the plot
plt.show()
```

This will give you the following scatterplots:
[INSERT IMAGE/PLOT8]


You'll see that this visualization confirms your classification report, which is very good news. :) 

### What's Next?

#### Digit Recognition in Natural Images 

Congratulations, you have reached the end of this tutorial that showed you how you can use supervised and unsupervised machine learning techniques on the `digits` data set. 

If you want to start your own machine learning project, you should not miss out on the MNIST data set, which you can download [here](http://yann.lecun.com/exdb/mnist/). 

The steps that you will need to take are very similar to the ones that you have gone through with this tutorial, but if you still feel that you can use some help, you should check out [this page](http://johnloeber.com/docs/kmeans.html), which works with the MNIST data and applies the KMeans algorithm. 

Working with the digits data set was the first step in classifying characters with scikit-learn. If you're done with this, you might consider trying out an even more challenging proble, namely, classifying alphanumeric characters in natural images.   

A well-known data set that you can use for this problem is the Chars74K dataset, which contains more than 74,000 images of digits from 0 to 9 and the both lowercase and highercase letters of the English alphabet, such as 'a' and 'A'. You can download the dataset [here](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/).

#### Data Visualization and `pandas` 

This tutorial was meant to introduce you to machine learning with Python. But this is definitely not the end of your journey of data science with Python: consider checking out our [Interactive Data Visualization with Bokeh course](https://www.datacamp.com/courses/interactive-data-visualization-with-bokeh) to go deeper into data visualization with Python or our [pandas Foundation course](https://www.datacamp.com/courses/pandas-foundations), to learn more about working with data frames in Python. 
